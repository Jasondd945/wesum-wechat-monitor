"""
AI æ–‡ç« å¤„ç†æ¨¡å—ï¼ˆæ•´åˆåˆ†ç±»å’Œæ‘˜è¦ï¼‰
ä½¿ç”¨é€šä¹‰åƒé—® API ç”Ÿæˆæ–‡ç« æ€»ç»“å’Œåˆ†ç±»æ ‡ç­¾
"""

import dashscope
from dashscope import Generation
from typing import Dict, List
import re


class AIArticleProcessor:
    """AI æ–‡ç« å¤„ç†å™¨ - æ•´åˆåˆ†ç±»åˆ¤æ–­å’Œæ‘˜è¦ç”Ÿæˆ"""

    def __init__(self, api_key: str, model: str = "qwen-turbo"):
        """
        åˆå§‹åŒ– AI æ–‡ç« å¤„ç†å™¨

        Args:
            api_key: é€šä¹‰åƒé—® API Key
            model: æ¨¡å‹åç§°
        """
        dashscope.api_key = api_key
        self.model = model
        self.noise_keywords = self._default_noise_keywords()

    def _default_noise_keywords(self) -> Dict[str, List[str]]:
        """é»˜è®¤çš„å¹²æ‰°å…³é”®è¯"""
        return {
            "æ‹›è˜": ["æ‹›è˜", "è¯šè˜", "çŒå¤´", "èŒä½", "JD", "ç®€å†", "åº”è˜", "é¢è¯•", "å…¥èŒ"],
            "å¸¦è´§": [
                # ä»·æ ¼ç›¸å…³ï¼ˆåˆ é™¤"å…ƒ"ç­‰è¿‡äºé€šç”¨çš„è¯ï¼‰
                "å—", "æ¯›", "æŠ˜", "ä¼˜æƒ ", "é™æ—¶", "ç‰¹ä»·", "æ¸…ä»“", "ç§’æ€", "æŠ¢è´­",
                "è´­ä¹°", "ä¸‹å•", "ç«‹å‡", "æ»¡å‡", "åˆ¸", "ç¦åˆ©", "çº¢åŒ…", "è¡¥è´´",
                # æ•°é‡/å•ä½ï¼ˆåˆ é™¤å•ä¸ªå­—æ¯Lå’Œmlï¼Œå®¹æ˜“è¯¯åˆ¤ï¼‰
                "æ”¯", "ä»¶", "å¥—", "ç›’", "ç“¶", "è¢‹", "æ–¤", "å…‹",
                # è¡ŒåŠ¨è¯
                "ç«‹å³æŠ¢", "é©¬ä¸ŠæŠ¢", "æŠ¢è´­", "é€ŸæŠ¢", "æ‰«ç ", "ç‚¹å‡»è´­ä¹°", "è´­ä¹°é“¾æ¥",
                # äº§å“æè¿°
                "çˆ†æ¬¾", "çƒ­é”€", "æ–°å“", "æ–°å“ä¸Šå¸‚", "é…ç½®æ‹‰æ»¡", "æ€§ä»·æ¯”", "è¶…å€¼",
                # å…¶ä»–
                "åŒ…é‚®", "è´§åˆ°ä»˜æ¬¾", "ä¸ƒå¤©é€€æ¢", "æ­£å“", "å®˜æ–¹", "æ——èˆ°åº—", "è‡ªè¥"
            ],
            "å¹¿å‘Š": ["èµåŠ©", "å¹¿å‘Š", "å“ç‰Œæ¨å¹¿", "å•†ä¸šåˆä½œ", "è½¯æ–‡", "æ¨å¹¿"],
            "è¯¾ç¨‹": ["è¯¾ç¨‹", "è®­ç»ƒè¥", "æ‰«ç ", "ç«‹å‡", "æŠ¥å", "å­¦ä¹ ", "åŸ¹è®­", "è®²åº§", "å…¬å¼€è¯¾"],
            "ç¤¾ç¾¤": ["çŸ¥è¯†æ˜Ÿçƒ", "ä»˜è´¹ç¤¾ç¾¤", "ä¼šå‘˜", "åŠ å…¥ç¤¾ç¾¤", "ç¤¾ç¾¤", "ç²‰ä¸ç¾¤", "äº¤æµç¾¤"],
            "æ´»åŠ¨æ¨å¹¿": ["ä¼šè®®æŠ¥å", "å±•ä¼šæŠ¥å", "æ—©é¸Ÿç¥¨", "æ´»åŠ¨æŠ¥å", "ç«‹å³æŠ¥å", "æŠ¥åå¼€å¯", "å¼€å¯æŠ¥å"],
            "èèµ„": ["èèµ„", "è½®èèµ„", "ä¼°å€¼", "å‹Ÿèµ„", "IPO", "ä¸Šå¸‚"],  # åˆ é™¤"æŠ•èµ„æ–¹"ä¸­çš„"æŠ•èµ„"
            "å…¬å…³": ["å‘å¸ƒ", "æ–°å“å‘å¸ƒ", "éš†é‡æ¨å‡º", "ç››å¤§å‘å¸ƒ", "æˆ˜ç•¥åˆä½œ", "ç­¾ç½²åè®®", "è·å¥–"]
        }

    def process_article(self, article: Dict) -> Dict:
        """
        å¤„ç†å•ç¯‡æ–‡ç« ï¼ˆåˆ¤æ–­å¹¿å‘Šç±»å‹ + ç”Ÿæˆæ‘˜è¦å’Œæ ‡ç­¾ï¼‰

        Args:
            article: æ–‡ç« ä¿¡æ¯ï¼ˆåŒ…å« title, content, author, link, publishedï¼‰

        Returns:
            {
                "summary": "æ‘˜è¦æ–‡æœ¬",
                "categories": ["æ ‡ç­¾1", "æ ‡ç­¾2"],  # AIç”Ÿæˆçš„ä¸»é¢˜åˆ†ç±»
                "is_noise": true/false,            # æ˜¯å¦ä¸ºå¹²æ‰°å†…å®¹
                "noise_type": "å¸¦è´§/æ‹›è˜/...",     # å¹²æ‰°ç±»å‹
                "noise_level": "noise/pr/light"    # å¹²æ‰°çº§åˆ«
            }
        """
        title = article.get('title', '')
        content = article.get('content', '')

        # æ­¥éª¤1: å…³é”®è¯åŒ¹é…,åˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Š
        noise_result = self._match_keywords(title + content)

        # æ­¥éª¤2: æ ¹æ®å¹¿å‘Šç±»å‹,é€‰æ‹©ä¸åŒçš„AIæç¤ºè¯
        if noise_result['is_noise']:
            # å¹¿å‘Šæ–‡ç« : ç”Ÿæˆç®€åŒ–æ‘˜è¦ + åˆ†ç±»æ ‡ç­¾
            result = self._generate_simple_summary(article, noise_result['noise_type'])
        else:
            # æ­£å¸¸æ–‡ç« : ç”Ÿæˆå®Œæ•´æ€»ç»“ + åˆ†ç±»æ ‡ç­¾
            result = self._generate_full_summary(article)

        # æ­¥éª¤3: åˆå¹¶ç»“æœ
        return {
            "summary": result['summary'],
            "categories": result['categories'],
            "is_noise": noise_result['is_noise'],
            "noise_type": noise_result['noise_type'],
            "noise_level": noise_result['noise_level']
        }

    def _match_keywords(self, text: str) -> Dict:
        """
        å…³é”®è¯åŒ¹é…åˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Š

        Args:
            text: æ–‡ç« æ ‡é¢˜+å†…å®¹

        Returns:
            {
                "is_noise": true/false,
                "noise_type": "å¸¦è´§/æ‹›è˜/...",
                "noise_level": "noise/pr/light"
            }
        """
        best_match_type = None
        best_match_count = 0

        for noise_type, keywords in self.noise_keywords.items():
            type_count = 0
            for keyword in keywords:
                if keyword in text:
                    type_count += 1

            if type_count > best_match_count:
                best_match_type = noise_type
                best_match_count = type_count

        # åŒ¹é…2ä¸ªä»¥ä¸Šå…³é”®è¯æ‰è®¤ä¸ºæ˜¯å¹¿å‘Š
        is_noise = best_match_count >= 2
        noise_level = self._get_noise_level(best_match_type) if is_noise else None

        return {
            "is_noise": is_noise,
            "noise_type": best_match_type if is_noise else None,
            "noise_level": noise_level
        }

    def _get_noise_level(self, noise_type: str) -> str:
        """è·å–å¹²æ‰°çº§åˆ«"""
        if noise_type in ["æ‹›è˜", "å¸¦è´§", "å¹¿å‘Š", "è¯¾ç¨‹", "ç¤¾ç¾¤", "æ´»åŠ¨æ¨å¹¿"]:
            return "noise"
        if noise_type in ["èèµ„", "å…¬å…³"]:
            return "pr"
        return "light"

    def _generate_full_summary(self, article: Dict) -> Dict:
        """
        ç”Ÿæˆæ­£å¸¸æ–‡ç« çš„å®Œæ•´æ€»ç»“ï¼ˆ500å­— + åˆ†ç±»æ ‡ç­¾ï¼‰

        Returns:
            {
                "summary": "æ€»ç»“æ–‡æœ¬",
                "categories": ["æ ‡ç­¾1", "æ ‡ç­¾2"]
            }
        """
        prompt = f"""è¯·å°†ä»¥ä¸‹å…¬ä¼—å·æ–‡ç« ç”Ÿæˆæ€»ç»“ï¼Œè¦æ±‚ï¼š

ã€æ ‡ç­¾ã€‘
1. è¾“å‡º3-5ä¸ªåˆ†ç±»æ ‡ç­¾ï¼ˆå…³é”®è¯ï¼‰
2. ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ï¼‰
3. æ ‡ç­¾ä¹‹é—´ç”¨é¡¿å·ã€åˆ†éš”
4. æ ‡ç­¾åº”è¯¥åæ˜ æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜ï¼ˆå¦‚ï¼šç§‘æŠ€ã€äº’è”ç½‘ã€å•†ä¸šåˆ†æç­‰ï¼‰

ã€æ€»ç»“ã€‘
1. **ç»“æ„åŒ–è¾“å‡º**ï¼šä½¿ç”¨ Emoji å›¾æ ‡ä½œä¸ºæ®µè½æ ‡è®°ï¼ˆå¦‚ğŸ¯ã€ğŸ”„ã€ğŸ¤–ã€ğŸ’¡ã€ğŸ“Šã€ğŸ”ç­‰ï¼‰
2. **åˆ†æ®µæ¸…æ™°**ï¼šæ¯ä¸ªå¤§æ®µæœ‰æ˜ç¡®çš„ä¸»é¢˜æ ‡é¢˜
3. **æ·±åº¦è§£æ**ï¼šä¸æ˜¯ç®€å•æ‘˜è¦ç‚¹ï¼Œè€Œæ˜¯ä¿ç•™å…³é”®ä¿¡æ¯å’Œæ•°æ®çš„æ·±åº¦è§£æ
4. **æ ¼å¼è§„èŒƒ**ï¼š
   - ä½¿ç”¨åˆ†çº§æ ‡é¢˜ï¼ˆä¸€ã€äºŒã€ä¸‰ï¼‰
   - å…³é”®æ•°æ®ç”¨åŠ ç²—æ ‡è®°
   - åŒ…å«å…·ä½“æ¡ˆä¾‹å’Œç»†èŠ‚
5. **å†…å®¹é•¿åº¦**ï¼šæ§åˆ¶åœ¨500å­—ä»¥å†…
6. **æ®µè½åˆ†éš”**ï¼šæ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
7. **è¡¥å……ç»†èŠ‚**ï¼šæœ€åè¡¥å……å…³é”®ç»†èŠ‚å’ŒèƒŒæ™¯ä¿¡æ¯

æ–‡ç« æ ‡é¢˜ï¼š{article['title']}

å…¬ä¼—å·ï¼š{article.get('author', 'Unknown')}

æ–‡ç« å†…å®¹ï¼š
{article.get('content', '')[:4000]}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

ã€æ ‡ç­¾ã€‘æ ‡ç­¾1ã€æ ‡ç­¾2ã€æ ‡ç­¾3

ã€æ€»ç»“ã€‘
ğŸ¯ ç¬¬ä¸€ä¸ªè¦ç‚¹çš„æ ‡é¢˜

ç¬¬ä¸€ä¸ªè¦ç‚¹çš„è¯¦ç»†å†…å®¹...

ğŸ”„ ç¬¬äºŒä¸ªè¦ç‚¹çš„æ ‡é¢˜

ç¬¬äºŒä¸ªè¦ç‚¹çš„è¯¦ç»†å†…å®¹...
"""

        try:
            response = Generation.call(
                model=self.model,
                prompt=prompt,
                max_tokens=1000
            )

            if response.status_code == 200:
                return self._parse_ai_response(response.output.text)
            else:
                return {
                    "summary": f"API é”™è¯¯: {response.code}",
                    "categories": []
                }

        except Exception as e:
            return {
                "summary": f"ç”Ÿæˆæ€»ç»“å¤±è´¥: {str(e)}",
                "categories": []
            }

    def _generate_simple_summary(self, article: Dict, noise_type: str) -> Dict:
        """
        ç”Ÿæˆå¹¿å‘Šæ–‡ç« çš„ç®€åŒ–æ‘˜è¦ï¼ˆ100å­— + åˆ†ç±»æ ‡ç­¾ï¼‰

        Args:
            article: æ–‡ç« ä¿¡æ¯
            noise_type: å¹¿å‘Šç±»å‹ï¼ˆå¸¦è´§ã€æ‹›è˜ç­‰ï¼‰

        Returns:
            {
                "summary": "ç®€åŒ–æ‘˜è¦",
                "categories": ["æ ‡ç­¾1", "æ ‡ç­¾2"]
            }
        """
        # æ ¹æ®å¹¿å‘Šç±»å‹å®šåˆ¶è¦ç‚¹è¦æ±‚
        points_requirements = {
            "æ‹›è˜": "- æ‹›è˜å…¬å¸\n- æ‹›è˜å²—ä½\n- è–ªèµ„èŒƒå›´\n- å·¥ä½œåœ°ç‚¹\n- å²—ä½è¦æ±‚",
            "å¸¦è´§": "- äº§å“åç§°\n- äº§å“ä»·æ ¼\n- ä¼˜æƒ ä¿¡æ¯\n- è´­ä¹°æ–¹å¼\n- æ´»åŠ¨æ—¶é—´",
            "å¹¿å‘Š": "- å“ç‰Œ/äº§å“\n- æ ¸å¿ƒä¿¡æ¯\n- æ¨å¹¿å†…å®¹",
            "è¯¾ç¨‹": "- è¯¾ç¨‹åç§°\n- è®²å¸ˆ/æœºæ„\n- è¯¾ç¨‹ä»·æ ¼\n- è¯¾ç¨‹æ—¶é•¿\n- æŠ¥åæ–¹å¼",
            "ç¤¾ç¾¤": "- ç¤¾ç¾¤åç§°\n- ç¤¾ç¾¤ç±»å‹\n- åŠ å…¥æ–¹å¼\n- è´¹ç”¨ä¿¡æ¯",
            "æ´»åŠ¨æ¨å¹¿": "- æ´»åŠ¨åç§°\n- æ´»åŠ¨æ—¶é—´\n- æ´»åŠ¨åœ°ç‚¹\n- ç¥¨ä»·ä¿¡æ¯\n- æŠ¥åæ–¹å¼",
            "èèµ„": "- èèµ„å…¬å¸\n- èèµ„è½®æ¬¡\n- èèµ„é‡‘é¢\n- æŠ•èµ„æ–¹\n- å…¬å¸ä¼°å€¼",
            "å…¬å…³": "- å…¬å¸/å“ç‰Œ\n- æ ¸å¿ƒä¿¡æ¯\n- å‘å¸ƒæ—¶é—´\n- ç›¸å…³æ•°æ®"
        }

        requirements = points_requirements.get(noise_type, "- è¦ç‚¹1\n- è¦ç‚¹2\n- è¦ç‚¹3")

        prompt = f"""è¯·å°†ä»¥ä¸‹å…¬ä¼—å·æ–‡ç« æå–ä¸ºå…³é”®è¦ç‚¹å’Œåˆ†ç±»æ ‡ç­¾ï¼Œè¦æ±‚ï¼š

ã€æ ‡ç­¾ã€‘
æ ¹æ®æ–‡ç« å†…å®¹ï¼Œç»™å‡º3-5ä¸ªåˆ†ç±»æ ‡ç­¾ï¼ˆå¦‚ï¼šç”µå•†ã€è´­ç‰©ã€èŒåœºã€æ•™è‚²ç­‰ï¼‰ï¼Œç”¨é¡¿å·ã€åˆ†éš”

ã€æ€»ç»“ã€‘
æç‚¼3-5ä¸ªå…³é”®è¦ç‚¹ï¼Œè¦æ±‚ï¼š
1. æ¯ä¸ªè¦ç‚¹ä¸è¶…è¿‡15å­—
2. ä¸¥æ ¼æ§åˆ¶åœ¨100å­—ä»¥å†…
3. å¿…é¡»åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
{requirements}

æ–‡ç« æ ‡é¢˜ï¼š{article['title']}

æ–‡ç« å†…å®¹ï¼š
{article.get('content', '')[:2000]}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

ã€æ ‡ç­¾ã€‘åˆ†ç±»1ã€åˆ†ç±»2ã€åˆ†ç±»3

ã€æ€»ç»“ã€‘
â€¢ è¦ç‚¹1
â€¢ è¦ç‚¹2
â€¢ è¦ç‚¹3"""

        try:
            response = Generation.call(
                model=self.model,
                prompt=prompt,
                max_tokens=300
            )

            if response.status_code == 200:
                return self._parse_ai_response(response.output.text)
            else:
                return {
                    "summary": f"API é”™è¯¯: {response.code}",
                    "categories": []
                }

        except Exception as e:
            return {
                "summary": f"ç”Ÿæˆç®€åŒ–æ‘˜è¦å¤±è´¥: {str(e)}",
                "categories": []
            }

    def _parse_ai_response(self, ai_text: str) -> Dict:
        """
        è§£æAIå“åº”,æå–æ ‡ç­¾å’Œæ‘˜è¦

        Args:
            ai_text: AIè¿”å›çš„æ–‡æœ¬

        Returns:
            {
                "summary": "æ‘˜è¦",
                "categories": ["æ ‡ç­¾1", "æ ‡ç­¾2"]
            }
        """
        categories = []
        summary = ai_text

        # æå–ã€æ ‡ç­¾ã€‘éƒ¨åˆ†
        tag_match = re.search(r'ã€æ ‡ç­¾ã€‘(.+?)(?=ã€æ€»ç»“ã€‘|\n\n|$)', ai_text)
        if tag_match:
            tag_text = tag_match.group(1).strip()
            categories = re.split(r'[ã€,ï¼Œ\s]+', tag_text)
            categories = list(set([c for c in categories if c.strip()]))
            categories = categories[:5]

        # æå–ã€æ€»ç»“ã€‘éƒ¨åˆ†
        summary_match = re.search(r'ã€æ€»ç»“ã€‘\s*\n(.+)', ai_text, re.DOTALL)
        if summary_match:
            summary = summary_match.group(1)
            # åªå»é™¤é¦–å°¾ç©ºè¡Œï¼Œä¿ç•™ä¸­é—´çš„æ®µè½ç»“æ„
            summary = summary.lstrip().rstrip()
        else:
            # å¦‚æœæ²¡æœ‰ã€æ€»ç»“ã€‘æ ‡è®°ï¼Œå»é™¤ã€æ ‡ç­¾ã€‘éƒ¨åˆ†
            summary = re.sub(r'ã€æ ‡ç­¾ã€‘.+', '', ai_text)
            summary = summary.lstrip().rstrip()

        return {
            "summary": summary,
            "categories": categories
        }
