# -*- coding: utf-8 -*-
"""
WeSum - å¾®ä¿¡å…¬ä¼—å·æ‘˜è¦æ¨é€åŠ©æ‰‹
ä¸»ç¨‹åºï¼šå¤šå…¬ä¼—å·è®¢é˜…ã€AIæ‘˜è¦ã€æ™ºèƒ½æ¨é€
"""

import sys
import io
import json
import os
import requests
from datetime import datetime, timedelta
from typing import Dict, List

# è®¾ç½® stdout ç¼–ç ä¸º UTF-8
if hasattr(sys.stdout, 'buffer'):
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)

# é€šä¹‰åƒé—® API
import dashscope
from dashscope import Generation

# ==================== é…ç½®åŠ è½½ ====================

# ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # python-dotenv æœªå®‰è£…ï¼Œè·³è¿‡

# é€šä¹‰åƒé—® API Keyï¼ˆå¿…éœ€ï¼‰
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEYï¼ˆåœ¨ .env æ–‡ä»¶ä¸­ï¼‰")

# ä¼ä¸šå¾®ä¿¡ Webhook URLï¼ˆå¿…éœ€ï¼‰
WEBHOOK_URL = os.getenv("WEBHOOK_URL")
if not WEBHOOK_URL:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ WEBHOOK_URLï¼ˆåœ¨ .env æ–‡ä»¶ä¸­ï¼‰")

# GitHub Tokenï¼ˆå¯é€‰ï¼Œç”¨äºåˆ›å»º Gistï¼‰
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")

# RSS Tokenï¼ˆå¯é€‰ï¼‰
RSS_TOKEN = os.getenv("RSS_TOKEN", "")

# å·²æ¨é€æ–‡ç« è®°å½•æ–‡ä»¶
SEEN_ARTICLES_FILE = "data/seen_articles.json"

# ==================== å…¬ä¼—å·è®¢é˜…é…ç½® ====================

def load_subscriptions():
    """
    ä» config.json åŠ è½½å…¬ä¼—å·è®¢é˜…é…ç½®

    ä¼˜å…ˆçº§ï¼šconfig.json > ç¯å¢ƒå˜é‡ > é»˜è®¤é…ç½®
    """
    # æ–¹æ¡ˆ 1: ä» config.json åŠ è½½ï¼ˆæ¨èï¼‰
    config_file = "config.json"

    if os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                subscriptions = config.get("rss_subscriptions", [])
                print(f"âœ… ä» config.json åŠ è½½äº† {len(subscriptions)} ä¸ªå…¬ä¼—å·é…ç½®")
                return subscriptions
        except Exception as e:
            print(f"âš ï¸ è¯»å– config.json å¤±è´¥: {e}")

    # æ–¹æ¡ˆ 2: ä»ç¯å¢ƒå˜é‡åŠ è½½ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    # æ ¼å¼ï¼šRSS_1_NAME, RSS_1_URL, RSS_1_ENABLED
    subscriptions = []
    i = 1
    while True:
        name = os.getenv(f"RSS_{i}_NAME")
        url = os.getenv(f"RSS_{i}_URL")
        enabled_str = os.getenv(f"RSS_{i}_ENABLED", "true")

        if not name or not url:
            break  # æ²¡æœ‰æ›´å¤šé…ç½®äº†

        subscriptions.append({
            "name": name,
            "url": url,
            "enabled": enabled_str.lower() == "true"
        })
        i += 1

    if subscriptions:
        print(f"âœ… ä»ç¯å¢ƒå˜é‡åŠ è½½äº† {len(subscriptions)} ä¸ªå…¬ä¼—å·é…ç½®")
        return subscriptions

    # æ–¹æ¡ˆ 3: é»˜è®¤ç¤ºä¾‹é…ç½®ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
    print("âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç¤ºä¾‹é…ç½®ï¼ˆè¯·åœ¨ config.json ä¸­é…ç½®ä½ çš„å…¬ä¼—å·ï¼‰")
    return [
        {
            "name": "ç¤ºä¾‹å…¬ä¼—å·1",
            "url": "https://wec.zeabur.app/feed/xxxxx.xml",
            "enabled": False
        },
    ]

# åŠ è½½å…¬ä¼—å·è®¢é˜…é…ç½®
RSS_SUBSCRIPTIONS = load_subscriptions()

# ==================== AI å¤„ç†å™¨ ====================

class AIArticleProcessor:
    """AI æ–‡ç« å¤„ç†å™¨"""

    def __init__(self, api_key: str, model: str = "qwen-turbo"):
        dashscope.api_key = api_key
        self.model = model
        self.noise_keywords = self._default_noise_keywords()

    def _default_noise_keywords(self) -> Dict[str, List[str]]:
        """å¹²æ‰°å…³é”®è¯é…ç½®"""
        return {
            "æ‹›è˜": [
                "è¯šè˜", "çƒ­æ‹›", "æ€¥è˜", "æ‹›è˜", "çŒå¤´", "æ‹›è´¤çº³å£«",
                "èŒä½æè¿°", "å²—ä½è¦æ±‚", "å²—ä½èŒè´£", "ä»»èŒè¦æ±‚",
                "æŠ•é€’ç®€å†", "å‘é€ç®€å†", "ç®€å†æŠ•é€’", "ç®€å†è¯·å‘",
                "è–ªèµ„å¾…é‡", "å¹´è–ª", "æœˆè–ª", "åº•è–ª", "è–ªèµ„é¢è®®", "äº”é™©ä¸€é‡‘"
            ],
            "å¸¦è´§": [
                "é™æ—¶ä¼˜æƒ ", "é™æ—¶ç‰¹æƒ ", "ç‰¹ä»·", "æ¸…ä»“", "ç§’æ€", "æŠ¢è´­", "å¤§ä¿ƒ",
                "ç«‹å‡", "æ»¡å‡", "ä¼˜æƒ å·", "ä¼˜æƒ åˆ¸", "é¢†åˆ¸", "æŠ˜æ‰£",
                "ç«‹å³è´­ä¹°", "é©¬ä¸ŠæŠ¢", "ç‚¹å‡»è´­ä¹°", "æ‰«ç è´­ä¹°", "è´­ä¹°é“¾æ¥",
                "ä¸‹å•é“¾æ¥", "æŠ¢è´­é“¾æ¥", "ç«‹å³ä¸‹å•", "é©¬ä¸ŠæŠ¢è´­",
                "çˆ†æ¬¾æ¨è", "çƒ­é”€çˆ†æ¬¾", "ç«çˆ†é”€å”®", "çƒ­é”€äº§å“", "ç•…é”€",
                "åŒ…é‚®", "è´§åˆ°ä»˜æ¬¾", "ä¸ƒå¤©é€€æ¢", "æ— ç†ç”±é€€æ¢", "æ­£å“ä¿è¯",
                "åŸä»·", "ç°ä»·", "ä¿ƒé”€ä»·", "æ´»åŠ¨ä»·", "ç‰¹ä»·"
            ],
            "è¯¾ç¨‹": [
                "è®­ç»ƒè¥æŠ¥å", "æ‰«ç æŠ¥å", "ç«‹å³æŠ¥å", "æŠ¥åé“¾æ¥", "å’¨è¯¢æŠ¥å",
                "è¯¾ç¨‹ä¼˜æƒ ", "é™æ—¶ç‰¹ä»·", "ç«‹å‡", "æ—©é¸Ÿä»·", "å›¢è´­ä»·",
                "åœ¨çº¿è¯¾ç¨‹", "è§†é¢‘è¯¾ç¨‹", "ç³»åˆ—è¯¾ç¨‹", "å®æˆ˜è¯¾ç¨‹", "ç³»ç»Ÿè¯¾ç¨‹",
                "åŒ…å­¦ä¼š", "å­¦ä¼šä¸ºæ­¢", "å…è´¹è¯•å¬", "è¯•å¬è¯¾ç¨‹"
            ],
            "ç¤¾ç¾¤": [
                "åŠ å…¥çŸ¥è¯†æ˜Ÿçƒ", "çŸ¥è¯†æ˜Ÿçƒ", "ä»˜è´¹ç¤¾ç¾¤", "ä»˜è´¹ç¤¾ç¾¤",
                "ä¼šå‘˜ä¸“åŒº", "VIPä¼šå‘˜", "ä¼šå‘˜æœåŠ¡", "æˆä¸ºä¼šå‘˜",
                "åŠ å…¥ç¤¾ç¾¤", "æ‰«ç åŠ ç¾¤", "ç²‰ä¸ç¾¤", "äº¤æµç¾¤", "ä»˜è´¹ç¾¤",
                "ç¤¾ç¾¤ç¦åˆ©", "ä¼šå‘˜ç¦åˆ©", "ä¸“å±ç¦åˆ©", "ä¼šå‘˜ä¸“äº«"
            ],
            "æ´»åŠ¨æ¨å¹¿": [
                "ä¼šè®®æŠ¥å", "å±•ä¼šæŠ¥å", "æ´»åŠ¨æŠ¥å", "æŠ¥åå¼€å¯", "æŠ¥åé€šé“",
                "æ—©é¸Ÿç¥¨", "æ—©é¸Ÿä¼˜æƒ ", "è´­ç¥¨é“¾æ¥", "æŠ¢ç¥¨", "é—¨ç¥¨",
                "åé¢æœ‰é™", "ä»…é™", "é™æ—¶å…è´¹", "é™æ—¶æŠ¥å"
            ],
            "èèµ„": [
                "è½®èèµ„", "å®Œæˆèèµ„", "è·å¾—èèµ„", "å‹Ÿèµ„å®Œæˆ",
                "ä¼°å€¼", "IPOä¸Šå¸‚", "å¯åŠ¨IPO", "æŒ‚ç‰Œä¸Šå¸‚"
            ],
            "å¹¿å‘Š": [
                "å¹¿å‘Šåˆä½œ", "å•†ä¸šåˆä½œ", "å“ç‰ŒèµåŠ©", "èµåŠ©å•†",
                "è½¯æ–‡æ¨å¹¿", "å“ç‰Œæ¨å¹¿", "äº§å“æ¨å¹¿", "å•†ä¸šæ¨å¹¿"
            ]
        }

    def detect_noise(self, title: str, content: str) -> tuple:
        """
        æ£€æµ‹æ–‡ç« æ˜¯å¦ä¸ºå¹²æ‰°å†…å®¹

        Returns:
            (noise_level, noise_type, matched_keywords)
            noise_level: None/light/heavy
            noise_type: å¹²æ‰°ç±»å‹ï¼ˆå¦‚"æ‹›è˜"ã€"å¸¦è´§"ç­‰ï¼‰
            matched_keywords: åŒ¹é…åˆ°çš„å…³é”®è¯åˆ—è¡¨
        """
        title_matches = {}
        content_matches = {}

        # æ£€æŸ¥æ ‡é¢˜ï¼ˆæƒé‡é«˜ï¼‰
        for noise_type, keywords in self.noise_keywords.items():
            matches = [kw for kw in keywords if kw in title]
            if matches:
                title_matches[noise_type] = matches

        # æ£€æŸ¥å†…å®¹
        for noise_type, keywords in self.noise_keywords.items():
            matches = [kw for kw in keywords if kw in content]
            if matches:
                content_matches[noise_type] = matches

        # åˆå¹¶ç»“æœï¼Œæ ‡é¢˜ä¼˜å…ˆ
        all_matches = {}
        for noise_type in set(list(title_matches.keys()) + list(content_matches.keys())):
            title_count = len(title_matches.get(noise_type, []))
            content_count = len(content_matches.get(noise_type, []))
            # æ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ˜¯å†…å®¹çš„2.5å€
            weighted_count = title_count * 2.5 + content_count
            if weighted_count >= 2:  # è‡³å°‘2ä¸ªåŠ æƒå…³é”®è¯
                all_matches[noise_type] = {
                    'title_kw': title_matches.get(noise_type, []),
                    'content_kw': content_matches.get(noise_type, []),
                    'weighted_count': weighted_count
                }

        if not all_matches:
            return None, None, []

        # æ‰¾åˆ°åŒ¹é…åº¦æœ€é«˜çš„ç±»å‹
        max_type = max(all_matches.keys(), key=lambda k: all_matches[k]['weighted_count'])
        max_count = all_matches[max_type]['weighted_count']

        # åˆ¤æ–­æ ‡å‡†
        if max_count >= 5 or (max_count >= 4 and len(all_matches[max_type]['title_kw']) >= 2):
            return "heavy", max_type, []
        elif max_count >= 2.5:
            return "light", max_type, []
        else:
            return None, None, []

    def generate_categories(self, title: str, content: str) -> List[str]:
        """ç”Ÿæˆæ–‡ç« åˆ†ç±»æ ‡ç­¾ï¼ˆä½¿ç”¨ AIï¼‰"""
        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆ2-3ä¸ªåˆ†ç±»æ ‡ç­¾ã€‚

æ ‡é¢˜ï¼š{title}

å†…å®¹ï¼š{content}

è¦æ±‚ï¼š
1. æ ‡ç­¾è¦ç®€æ´ï¼Œ2-4ä¸ªå­—
2. æ ‡ç­¾è¦å‡†ç¡®åæ˜ æ–‡ç« ä¸»é¢˜
3. å¸¸è§æ ‡ç­¾åŒ…æ‹¬ï¼šAIã€ç§‘æŠ€ã€å‰ç«¯ã€äº§å“ã€ç®¡ç†ã€ç®—æ³•ã€æŠ€æœ¯è¶‹åŠ¿ç­‰
4. ç›´æ¥è¿”å›æ ‡ç­¾ï¼Œç”¨é¡¿å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–è¯´æ˜"""

        try:
            response = Generation.call(
                model=self.model,
                prompt=prompt,
                max_tokens=100,
                temperature=0.3
            )

            if response.status_code == 200:
                categories_text = response.output.text.strip()
                categories = [c.strip() for c in categories_text.split('ã€') if c.strip()]
                return categories[:3]
        except Exception as e:
            print(f"åˆ†ç±»ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")

        return []

    def summarize_article(self, content: str, title: str = "", author: str = "") -> str:
        """
        ä½¿ç”¨ AI ç”Ÿæˆæ–‡ç« æ‘˜è¦

        Args:
            content: æ–‡ç« å†…å®¹
            title: æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            author: å…¬ä¼—å·åç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        # æˆªå–å†…å®¹ï¼ˆé¿å…è¶…å‡º token é™åˆ¶ï¼‰
        if len(content) > 4000:
            content = content[:4000]

        prompt = f"""è¯·å°†ä»¥ä¸‹å…¬ä¼—å·æ–‡ç« ç”Ÿæˆæ€»ç»“ï¼Œè¦æ±‚ï¼š

ã€æ ‡ç­¾ã€‘
1. è¾“å‡º3-5ä¸ªåˆ†ç±»æ ‡ç­¾ï¼ˆå…³é”®è¯ï¼‰
2. ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ï¼‰
3. æ ‡ç­¾ä¹‹é—´ç”¨é¡¿å·ã€åˆ†éš”
4. æ ‡ç­¾åº”è¯¥åæ˜ æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜ï¼ˆå¦‚ï¼šç§‘æŠ€ã€äº’è”ç½‘ã€å•†ä¸šåˆ†æç­‰ï¼‰

ã€æ€»ç»“ã€‘
1. **ç»“æ„åŒ–è¾“å‡º**ï¼šä½¿ç”¨ Emoji å›¾æ ‡ä½œä¸ºæ®µè½æ ‡è®°ï¼ˆå¦‚ğŸ¯ã€ğŸ”„ã€ğŸ¤–ã€ğŸ’¡ã€ğŸ“Šã€ğŸ”ç­‰ï¼‰
2. **åˆ†æ®µæ¸…æ™°**ï¼š3-5ä¸ªå¤§æ®µï¼Œæ¯ä¸ªå¤§æ®µæœ‰æ˜ç¡®çš„ä¸»é¢˜æ ‡é¢˜
3. **æ·±åº¦è§£æ**ï¼šä¸æ˜¯ç®€å•æ‘˜è¦ç‚¹ï¼Œè€Œæ˜¯ä¿ç•™å…³é”®ä¿¡æ¯å’Œæ•°æ®çš„æ·±åº¦è§£æ
4. **æ ¼å¼è§„èŒƒ**ï¼š
   - ä½¿ç”¨åˆ†çº§æ ‡é¢˜ï¼ˆä¸€ã€äºŒã€ä¸‰ï¼‰
   - æ‰€æœ‰æ ‡é¢˜å¿…é¡»åŠ ç²—ï¼ˆä½¿ç”¨ **æ ‡é¢˜** æ ¼å¼ï¼‰
   - å…³é”®æ•°æ®ç”¨åŠ ç²—æ ‡è®°
   - åŒ…å«å…·ä½“æ¡ˆä¾‹å’Œç»†èŠ‚
5. **å†…å®¹é•¿åº¦**ï¼šæ§åˆ¶åœ¨500å­—ä»¥å†…
6. **æ®µè½åˆ†éš”**ï¼šæ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
7. **è¡¥å……ç»†èŠ‚**ï¼šæœ€åè¡¥å……å…³é”®ç»†èŠ‚å’ŒèƒŒæ™¯ä¿¡æ¯ï¼ˆ"è¡¥å……ç»†èŠ‚"æˆ–è€…"å…³é”®ç»†èŠ‚è¡¥å……"ä¹Ÿè¦åŠ ç²—ï¼Œä½¿ç”¨ **è¡¥å……ç»†èŠ‚** æ ¼å¼ï¼‰

æ–‡ç« æ ‡é¢˜ï¼š{title}

å…¬ä¼—å·ï¼š{author}

æ–‡ç« å†…å®¹ï¼š
{content}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

ã€æ ‡ç­¾ã€‘æ ‡ç­¾1ã€æ ‡ç­¾2ã€æ ‡ç­¾3

ã€æ€»ç»“ã€‘
ğŸ¯ **ç¬¬ä¸€ä¸ªè¦ç‚¹çš„æ ‡é¢˜**

ç¬¬ä¸€ä¸ªè¦ç‚¹çš„è¯¦ç»†å†…å®¹...

ğŸ”„ **ç¬¬äºŒä¸ªè¦ç‚¹çš„æ ‡é¢˜**

ç¬¬äºŒä¸ªè¦ç‚¹çš„è¯¦ç»†å†…å®¹...

ç¬¬ä¸‰åˆ°ç¬¬äº”ä¸ªç±»ä¼¼ä¸Šé¢ç»§ç»­

ğŸ’¡ **è¡¥å……ç»†èŠ‚**

å…³é”®ç»†èŠ‚å’ŒèƒŒæ™¯ä¿¡æ¯...
"""

        try:
            response = Generation.call(
                model=self.model,
                prompt=prompt,
                max_tokens=1000,
                temperature=0.5
            )

            if response.status_code == 200:
                ai_text = response.output.text.strip()

                # æå–ã€æ€»ç»“ã€‘éƒ¨åˆ†
                import re
                summary_match = re.search(r'ã€æ€»ç»“ã€‘\s*\n(.+)', ai_text, re.DOTALL)
                if summary_match:
                    summary = summary_match.group(1)
                    summary = summary.lstrip().rstrip()
                    return summary
                else:
                    # å¦‚æœæ²¡æœ‰ã€æ€»ç»“ã€‘æ ‡è®°ï¼Œå»é™¤ã€æ ‡ç­¾ã€‘éƒ¨åˆ†
                    summary = re.sub(r'ã€æ ‡ç­¾ã€‘.+', '', ai_text)
                    return summary.lstrip().rstrip()

        except Exception as e:
            print(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
            return content[:200] + "..."

        return content[:200] + "..."


# ==================== è¾…åŠ©å‡½æ•° ====================

def load_seen_articles() -> set:
    """åŠ è½½å·²æ¨é€æ–‡ç« çš„é“¾æ¥é›†åˆ"""
    if os.path.exists(SEEN_ARTICLES_FILE):
        with open(SEEN_ARTICLES_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return set(data.get('seen_links', []))
    return set()


def save_seen_articles(seen_links: set):
    """ä¿å­˜å·²æ¨é€æ–‡ç« çš„é“¾æ¥é›†åˆ"""
    os.makedirs(os.path.dirname(SEEN_ARTICLES_FILE), exist_ok=True)
    with open(SEEN_ARTICLES_FILE, 'w', encoding='utf-8') as f:
        json.dump({
            'seen_links': list(seen_links),
            'updated_at': datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)


def format_published_time(published: str) -> str:
    """
    æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´ä¸º -YYYY-MM-DD HH:MM æ ¼å¼

    Args:
        published: RSS ä¸­çš„ published å­—æ®µï¼ˆå¦‚ "Mon, 12 Jan 2026 12:36:00 +0800"ï¼‰

    Returns:
        æ ¼å¼åŒ–åçš„æ—¶é—´å­—ç¬¦ä¸²ï¼ˆå¦‚ "-2026-01-12 12:36"ï¼‰
    """
    if not published or published == 'Unknown':
        return ""

    try:
        from email.utils import parsedate_to_datetime
        dt = parsedate_to_datetime(published)
        return f"-{dt.strftime('%Y-%m-%d %H:%M')}"
    except:
        return ""


def parse_published_time(published: str) -> datetime:
    """è§£æå‘å¸ƒæ—¶é—´ä¸º datetime å¯¹è±¡ï¼ˆç”¨äºæ’åºï¼‰"""
    if not published or published == 'Unknown':
        return datetime.min

    try:
        from email.utils import parsedate_to_datetime
        return parsedate_to_datetime(published)
    except:
        return datetime.min


def _is_within_time_range(entry, time_threshold: datetime) -> bool:
    """æ£€æŸ¥æ–‡ç« å‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…"""
    from email.utils import parsedate_to_datetime

    try:
        published_str = entry.get('published') or entry.get('updated', '')
        if not published_str:
            return False  # æ— æ³•è§£ææ—¶é—´æ—¶é»˜è®¤è¿‡æ»¤

        dt = parsedate_to_datetime(published_str)

        # å¦‚æœ dt æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå°† time_threshold ä¹Ÿè½¬æ¢ä¸ºå¸¦æ—¶åŒºçš„æ—¶é—´
        if dt.tzinfo is not None and time_threshold.tzinfo is None:
            from datetime import timezone
            time_threshold = time_threshold.replace(tzinfo=timezone.utc)

        return dt >= time_threshold
    except:
        return False  # è§£æå¤±è´¥æ—¶é»˜è®¤è¿‡æ»¤


def fetch_rss_articles(url, seen_links: set = None, max_hours: int = 24):
    """
    ä» Zeabur è·å– RSS æ–‡ç« åˆ—è¡¨ï¼ˆå¸¦è®°å¿†æœºåˆ¶ï¼‰

    Args:
        url: RSS åœ°å€
        seen_links: å·²æ¨é€æ–‡ç« é“¾æ¥é›†åˆ
        max_hours: åªè·å–æœ€è¿‘ N å°æ—¶å†…çš„æ–‡ç« ï¼ˆé»˜è®¤24å°æ—¶ï¼‰

    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    if seen_links is None:
        seen_links = set()

    print(f"æ­£åœ¨è·å– RSSï¼š{url}")

    import feedparser
    import re

    try:
        feed = feedparser.parse(url)
        time_threshold = datetime.now() - timedelta(hours=max_hours)

        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“° å…¬ä¼—å·ï¼š{feed.feed.get('title', 'Unknown')}")
        print(f"ğŸ“Š RSS æ–‡ç« æ€»æ•°ï¼š{len(feed.entries)}")
        print(f"â° æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘ {max_hours} å°æ—¶")
        print()

        # æå–æ–‡ç« ä¿¡æ¯
        articles = []
        new_count = 0
        skipped_seen = 0
        skipped_time = 0

        for idx, entry in enumerate(feed.entries, 1):
            article = {
                'title': entry.get('title', 'æ— æ ‡é¢˜'),
                'link': entry.get('link', ''),
                'author': feed.feed.get('title', 'Unknown'),
                'published': entry.get('published', entry.get('updated', '')),
                'content': ''
            }

            # æ£€æŸ¥1: æ˜¯å¦å·²æ¨é€
            if article['link'] in seen_links:
                skipped_seen += 1
                continue

            # æ£€æŸ¥2: æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
            is_within_time = _is_within_time_range(entry, time_threshold)
            if not is_within_time:
                skipped_time += 1
                continue

            # æå–å†…å®¹
            if hasattr(entry, 'content') and entry.content:
                article['content'] = entry.content[0].value
            elif hasattr(entry, 'summary'):
                article['content'] = entry.summary
            elif hasattr(entry, 'description'):
                article['content'] = entry.description

            # å»é™¤ HTML æ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬
            article['content'] = re.sub(r'<[^>]+>', '', article['content'])
            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(article['content']) > 2000:
                article['content'] = article['content'][:2000]

            articles.append(article)
            new_count += 1

            if new_count <= 3:  # æ˜¾ç¤ºå‰3ç¯‡æ–°æ–‡ç« çš„è¯¦æƒ…
                print(f"   âœ… æ–°æ–‡ç« ï¼š{article['title'][:50]}...")
                print(f"      é“¾æ¥ï¼š{article['link'][:80]}...")
                print(f"      æ—¶é—´ï¼š{article['published']}")

        print(f"   ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"      - æ€»æ–‡ç« æ•°ï¼š{len(feed.entries)}")
        print(f"      - å·²æ¨é€ï¼ˆè·³è¿‡ï¼‰ï¼š{skipped_seen}")
        print(f"      - è¶…æ—¶ï¼ˆè·³è¿‡ï¼‰ï¼š{skipped_time}")
        print(f"      - æ–°æ–‡ç« ï¼š{new_count}")
        print(f"   âœ… è·å–åˆ° {new_count} ç¯‡æ–°æ–‡ç« ")
        return articles

    except Exception as e:
        print(f"âŒ è·å– RSS å¤±è´¥ï¼š{str(e)}")
        return []


# ==================== Gist ç›¸å…³å‡½æ•° ====================

def create_gist(content, account_name, github_token):
    """
    åˆ›å»º GitHub Gist

    Args:
        content: è¦å­˜å‚¨çš„å†…å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰
        account_name: å…¬ä¼—å·åç§°ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
        github_token: GitHub Personal Access Token

    Returns:
        Gist çš„ HTML URLï¼ˆå¦‚æœæˆåŠŸï¼‰
        Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    filename = f"{account_name}_æ‘˜è¦_{timestamp}.md"

    # Gist API ç«¯ç‚¹
    url = "https://api.github.com/gists"

    # æ„å»ºè¯·æ±‚ä½“
    data = {
        "description": f"{account_name} å…¬ä¼—å·æ–‡ç« æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "public": False,  # ç§æœ‰ Gist
        "files": {
            filename: {
                "content": content
            }
        }
    }

    # è¯·æ±‚å¤´
    headers = {
        "Authorization": f"token {github_token}",
        "Accept": "application/vnd.github.v3+json"
    }

    try:
        print(f"ğŸ“¤ æ­£åœ¨åˆ›å»º GitHub Gist...")
        response = requests.post(url, json=data, headers=headers, timeout=10)

        if response.status_code == 201:
            gist_data = response.json()
            gist_url = gist_data['html_url']
            print(f"âœ… Gist åˆ›å»ºæˆåŠŸ!")
            print(f"   URL: {gist_url}")
            return gist_url
        else:
            print(f"âŒ Gist åˆ›å»ºå¤±è´¥: HTTP {response.status_code}")
            print(f"   é”™è¯¯ä¿¡æ¯: {response.text}")
            return None

    except Exception as e:
        print(f"âŒ åˆ›å»º Gist æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return None


def format_push_message_for_gist(articles, title="å…¬ä¼—å·æ–‡ç« æ‘˜è¦æ±‡æ€»"):
    """
    æ ¼å¼åŒ–æ¨é€æ¶ˆæ¯ï¼ˆç”¨äºå­˜å‚¨åˆ° Gistï¼‰

    Args:
        articles: æ–‡ç« åˆ—è¡¨
        title: æ±‡æ€»æ ‡é¢˜

    Returns:
        å®Œæ•´çš„æ–‡ç« æ‘˜è¦æ–‡æœ¬ï¼ˆMarkdown æ ¼å¼ï¼‰
    """
    # ä½¿ç”¨åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
    now = (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M')

    # ç»Ÿè®¡å…¬ä¼—å·æ•°é‡
    account_names = set(article.get('author', '') for article in articles if article.get('author'))
    account_count = len(account_names)

    # æ„å»ºå®Œæ•´çš„ Markdown å†…å®¹
    if account_count > 1:
        content = f"""# ğŸ“° {title} ({now})

ğŸ“Š è®¢é˜…å…¬ä¼—å·ï¼š{account_count} ä¸ª

---

"""
    else:
        content = f"""# ğŸ“° {title} ({now})

---

"""

    # ç»Ÿè®¡å„ç±»æ–‡ç« æ•°é‡
    normal_count = 0
    light_noise_count = 0
    heavy_noise_count = 0

    for i, article in enumerate(articles, 1):
        noise_level = article.get('noise_level')
        noise_type = article.get('noise_type')

        # ç»Ÿè®¡æ–‡ç« ç±»å‹
        if noise_level == "heavy":
            heavy_noise_count += 1
        elif noise_level == "light":
            light_noise_count += 1
        else:
            normal_count += 1

        # æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´
        published_time = format_published_time(article.get('published', ''))

        # æ ‡é¢˜ï¼ˆåŒ…å«å‘å¸ƒæ—¶é—´ï¼‰
        if article.get('author'):
            content += f"## {i}. ã€{article['author']}ã€‘{article['title']}{published_time}\n\n"
        else:
            content += f"## {i}. {article['title']}{published_time}\n\n"

        # åˆ†ç±»æ ‡ç­¾
        if article.get('categories'):
            category_str = "ã€".join(article['categories'])
            content += f"**ğŸ·ï¸ åˆ†ç±»**ï¼š{category_str}\n\n"

        # æ ¹æ®å™ªéŸ³çº§åˆ«æ·»åŠ ä¸åŒçš„æç¤ºå’Œæ‘˜è¦
        if noise_level == "heavy":
            content += f"**ğŸ“¢ {noise_type}å†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼‰**\n\n"
            if article.get('ai_summary'):
                content += f"{article['ai_summary']}\n\n"
        elif noise_level == "light":
            content += f"**âš ï¸ å¯èƒ½åŒ…å«{noise_type}å†…å®¹**\n\n"
            if article.get('ai_summary'):
                content += f"{article['ai_summary']}\n\n"
        else:
            if article.get('ai_summary'):
                content += f"{article['ai_summary']}\n\n"

        # åŸæ–‡é“¾æ¥
        content += f"**ğŸ”— æŸ¥çœ‹åŸæ–‡**ï¼š{article['link']}\n\n"
        content += "---\n\n"

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    total_articles = normal_count + light_noise_count + heavy_noise_count

    content += f"""
## ğŸ“Š æ•°æ®ç»Ÿè®¡

- **æ­£å¸¸æ–‡ç« **ï¼š{normal_count} ç¯‡
- **è½»åº¦å¹²æ‰°**ï¼š{light_noise_count} ç¯‡
- **é‡åº¦å¹²æ‰°**ï¼š{heavy_noise_count} ç¯‡ï¼ˆç®€åŒ–æ˜¾ç¤ºï¼‰
- **æ€»è®¡**ï¼š{total_articles} ç¯‡

---

*Generated by WeSum AI æ‘˜è¦åŠ©æ‰‹*
"""

    return content


# ==================== ä¼ä¸šå¾®ä¿¡æ¨é€å‡½æ•° ====================

def send_to_wechat_with_gist_link(account_name, gist_url, webhook_url, articles):
    """
    å‘é€ä¼ä¸šå¾®ä¿¡å¡ç‰‡æ¶ˆæ¯ï¼ˆåŒ…å«æ–‡ç« åˆ—è¡¨å’Œ Gist é“¾æ¥ï¼‰

    Args:
        account_name: å…¬ä¼—å·åç§°
        gist_url: Gist é“¾æ¥
        webhook_url: ä¼ä¸šå¾®ä¿¡ webhook åœ°å€
        articles: æ–‡ç« åˆ—è¡¨
    """
    # ä½¿ç”¨åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
    now = (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M')

    # æ„å»ºç®€æ´çš„æ–‡ç« åˆ—è¡¨ï¼ˆåªåŒ…å«æ ‡é¢˜å’Œé“¾æ¥ï¼‰
    article_list = ""
    for i, article in enumerate(articles, 1):
        published_time = format_published_time(article.get('published', ''))
        author = article.get('author', '')
        author_tag = f"ã€{author}ã€‘" if author else ""
        article_list += f"{i}. {author_tag}[{article['title']}]({article['link']}){published_time}\n"

    message = {
        "msgtype": "markdown",
        "markdown": {
            "content": f"""# ğŸ“° å…¬ä¼—å·æ–‡ç« æ›´æ–°

**å…¬ä¼—å·**: {account_name}
**æ›´æ–°æ—¶é—´**: {now}
**æ–‡ç« æ•°é‡**: {len(articles)} ç¯‡

ğŸ‘‰ **[ç‚¹å‡»æŸ¥çœ‹å®Œæ•´æ‘˜è¦]({gist_url})**

----
**ğŸ“ æ–‡ç« åˆ—è¡¨**:
-{article_list}

----
<font color="info">WeSum AI æ‘˜è¦åŠ©æ‰‹</font>
"""
        }
    }

    try:
        print(f"ğŸ“¤ æ­£åœ¨å‘é€ä¼ä¸šå¾®ä¿¡å¡ç‰‡æ¶ˆæ¯...")
        response = requests.post(webhook_url, json=message, timeout=10)

        if response.status_code == 200:
            result = response.json()
            if result.get('errcode') == 0:
                print(f"âœ… ä¼ä¸šå¾®ä¿¡æ¨é€æˆåŠŸ!")
                return True
            else:
                print(f"âŒ ä¼ä¸šå¾®ä¿¡æ¨é€å¤±è´¥: {result.get('errmsg')}")
                return False
        else:
            print(f"âŒ ä¼ä¸šå¾®ä¿¡ API é”™è¯¯: HTTP {response.status_code}")
            return False

    except Exception as e:
        print(f"âŒ å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False


def send_no_new_articles_message(webhook_url):
    """
    å‘é€æ— æ–°æ–‡ç« é€šçŸ¥

    Args:
        webhook_url: ä¼ä¸šå¾®ä¿¡ webhook åœ°å€
    """
    # ä½¿ç”¨åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
    now = (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M')

    message = {
        "msgtype": "markdown",
        "markdown": {
            "content": f"""# ğŸ“° å…¬ä¼—å·æ–‡ç« æ›´æ–°

**å…¬ä¼—å·**: æœ¬æ¬¡æ— æ–°æ–‡ç« æ¨é€
**æ›´æ–°æ—¶é—´**: {now}

---
<font color="info">WeSum AI æ‘˜è¦åŠ©æ‰‹</font>
"""
        }
    }

    try:
        print(f"ğŸ“¤ æ­£åœ¨å‘é€æ— æ–°æ–‡ç« é€šçŸ¥...")
        response = requests.post(webhook_url, json=message, timeout=10)

        if response.status_code == 200:
            result = response.json()
            if result.get('errcode') == 0:
                print(f"âœ… é€šçŸ¥å‘é€æˆåŠŸ!")
                return True
            else:
                print(f"âŒ é€šçŸ¥å‘é€å¤±è´¥: {result.get('errmsg')}")
                return False
        else:
            print(f"âŒ ä¼ä¸šå¾®ä¿¡ API é”™è¯¯: HTTP {response.status_code}")
            return False

    except Exception as e:
        print(f"âŒ å‘é€é€šçŸ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False


# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("WeSum - å¾®ä¿¡å…¬ä¼—å·æ‘˜è¦æ¨é€åŠ©æ‰‹")
    print("=" * 60)
    print(f"å¯åŠ¨æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    print("å·¥ä½œæµç¨‹ï¼š")
    print("1. åŠ è½½å·²æ¨é€æ–‡ç« è®°å¿†")
    print("2. ä»å¤šä¸ªå…¬ä¼—å·è·å–æ–°æ–‡ç« ï¼ˆæœ€è¿‘24å°æ—¶ï¼‰")
    print("3. AI å¹²æ‰°æ–‡ç« è¯†åˆ«")
    print("4. AI åˆ†ç±»æ ‡ç­¾ç”Ÿæˆ")
    print("5. AI æ‘˜è¦ç”Ÿæˆ")
    print("6. åˆ›å»º GitHub Gist")
    print("7. æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡")
    print("8. ä¿å­˜å·²æ¨é€æ–‡ç« è®°å¿†")
    print()
    print("=" * 60)
    print()

    # 1. åŠ è½½å·²æ¨é€æ–‡ç« è®°å¿†
    print("[Step 1] åŠ è½½å·²æ¨é€æ–‡ç« è®°å¿†...")
    seen_links = load_seen_articles()
    print(f"   âœ… å·²è®°å½• {len(seen_links)} ç¯‡æ–‡ç« ")
    print()

    # 2. ç­›é€‰å¯ç”¨çš„è®¢é˜…
    active_subscriptions = [sub for sub in RSS_SUBSCRIPTIONS if sub.get('enabled', True)]

    if not active_subscriptions:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„å…¬ä¼—å·è®¢é˜…")
        exit(1)

    print(f"[Step 2] è®¢é˜…é…ç½®ï¼š{len(active_subscriptions)} ä¸ªå…¬ä¼—å·")
    for sub in active_subscriptions:
        print(f"   - {sub['name']}")
    print()

    # 3. ä»å¤šä¸ªå…¬ä¼—å·è·å–æ–‡ç« 
    print("[Step 3] ä»å¤šä¸ªå…¬ä¼—å·è·å–æ–°æ–‡ç« ...")
    all_articles = []

    for subscription in active_subscriptions:
        account_name = subscription['name']
        rss_url = subscription['url']

        print(f"\næ­£åœ¨è·å–ã€{account_name}ã€‘çš„æ–‡ç« ...")
        articles = fetch_rss_articles(rss_url, seen_links=seen_links, max_hours=24)

        if articles:
            all_articles.extend(articles)
        else:
            print(f"   âš ï¸ æ— æ–°æ–‡ç« ")

    if not all_articles:
        print("\nâŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ–°æ–‡ç« ")

        # æ£€æŸ¥å½“å‰æ—¶é—´æ˜¯å¦åœ¨é™é»˜æ—¶æ®µï¼ˆ0-9ç‚¹ï¼‰
        current_hour = datetime.now().hour
        if 0 <= current_hour < 9:
            print(f"\nâ° å½“å‰æ—¶é—´ {current_hour}:00 å¤„äºé™é»˜æ—¶æ®µï¼ˆ0-9ç‚¹ï¼‰ï¼Œè·³è¿‡ç©ºæ¶ˆæ¯æ¨é€")
            print("\nâœ… è¿è¡Œå®Œæˆï¼ˆé™é»˜æ—¶æ®µï¼Œæ— æ–°æ–‡ç« ï¼‰")
            exit(0)

        print("\n[Step 4] å‘é€æ— æ–°æ–‡ç« é€šçŸ¥...")
        send_no_new_articles_message(WEBHOOK_URL)
        print("\nâœ… è¿è¡Œå®Œæˆï¼ˆæ— æ–°æ–‡ç« éœ€è¦å¤„ç†ï¼‰")
        exit(0)

    print(f"\nğŸ“Š æ€»è®¡è·å– {len(all_articles)} ç¯‡æ–°æ–‡ç« ")

    # 4. æŒ‰å‘å¸ƒæ—¶é—´é™åºæ’åº
    print("\n[Step 4] æŒ‰å‘å¸ƒæ—¶é—´é™åºæ’åº...")
    all_articles.sort(key=lambda a: parse_published_time(a.get('published', '')), reverse=True)
    print(f"   âœ… æ’åºå®Œæˆ")
    print()
    print("=" * 60)
    print()

    # åˆ›å»º AI å¤„ç†å™¨
    ai_processor = AIArticleProcessor(api_key=DASHSCOPE_API_KEY)

    processed_articles = []

    # å¤„ç†æ¯ç¯‡æ–‡ç« 
    for i, article in enumerate(all_articles, 1):
        print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(all_articles)} ç¯‡æ–‡ç« ...")
        print(f"æ ‡é¢˜: {article['title']}")
        print(f"å…¬ä¼—å·: {article.get('author', 'Unknown')}")
        print()

        # æ­¥éª¤ 1: å¹²æ‰°æ–‡ç« è¯†åˆ«
        print("  [1/3] è¯†åˆ«å¹²æ‰°å†…å®¹...")
        noise_level, noise_type, matched_keywords = ai_processor.detect_noise(
            article['title'],
            article['content']
        )
        article['noise_level'] = noise_level
        article['noise_type'] = noise_type

        if noise_level:
            print(f"       æ£€æµ‹åˆ°ï¼š{noise_type} ({noise_level})")
        else:
            print(f"       âœ… æ­£å¸¸æ–‡ç« ")

        # å¦‚æœæ˜¯é‡åº¦å¹²æ‰°ï¼Œè·³è¿‡åç»­å¤„ç†
        if noise_level == "heavy":
            print(f"       âš ï¸ å·²è¿‡æ»¤ï¼Œä¸è¿›è¡Œåç»­å¤„ç†")
            processed_articles.append(article)
            print()
            continue

        # æ­¥éª¤ 2: ç”Ÿæˆåˆ†ç±»æ ‡ç­¾
        print("  [2/3] ç”Ÿæˆåˆ†ç±»æ ‡ç­¾...")
        categories = ai_processor.generate_categories(
            article['title'],
            article['content']
        )
        article['categories'] = categories
        print(f"       æ ‡ç­¾ï¼š{'ã€'.join(categories) if categories else 'æœªç”Ÿæˆ'}")

        # æ­¥éª¤ 3: ç”Ÿæˆ AI æ‘˜è¦
        print("  [3/3] ç”Ÿæˆ AI æ‘˜è¦...")
        summary = ai_processor.summarize_article(
            article['content'],
            article['title']
        )
        article['ai_summary'] = summary
        print(f"       âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ")

        # æ‰“å°å®Œæ•´çš„ AI æ‘˜è¦å†…å®¹ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        print()
        print("       ğŸ“ æ‘˜è¦å†…å®¹ï¼š")
        print("       " + "=" * 56)
        # æ‰“å°æ‘˜è¦ï¼Œæ¯è¡Œå‰é¢åŠ ç¼©è¿›
        for line in summary.split('\n'):
            print(f"       {line}")
        print("       " + "=" * 56)

        processed_articles.append(article)
        print()

    if not processed_articles:
        print("âŒ æ²¡æœ‰å¯æ¨é€çš„æ–‡ç« ")
        exit(0)

    # æ ¼å¼åŒ– Gist å†…å®¹
    print("=" * 60)
    print("æ ¼å¼åŒ– Gist å†…å®¹")
    print("=" * 60)
    print()

    # ç”Ÿæˆæ±‡æ€»æ ‡é¢˜
    account_names = set(article.get('author', '') for article in processed_articles if article.get('author'))
    account_count = len(account_names)
    article_count = len(processed_articles)

    summary_title = f"æ–‡ç« æ‘˜è¦æ±‡æ€»ã€{account_count}ä¸ªå…¬ä¼—å·ã€{article_count}ç¯‡æ–‡ç« ã€‘"

    gist_content = format_push_message_for_gist(processed_articles, summary_title)

    print(f"âœ… Gist å†…å®¹æ ¼å¼åŒ–å®Œæˆ")
    print(f"å†…å®¹é•¿åº¦: {len(gist_content)} å­—ç¬¦")
    print()

    # åˆ›å»º GitHub Gist
    print("=" * 60)
    print("åˆ›å»º GitHub Gist")
    print("=" * 60)
    print()

    gist_url = create_gist(gist_content, summary_title, GITHUB_TOKEN)

    if not gist_url:
        print("âŒ Gist åˆ›å»ºå¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
        exit(1)

    print()

    # æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆä½¿ç”¨ Gist é“¾æ¥ï¼‰
    print()
    print("=" * 60)
    print("æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆGist é“¾æ¥ï¼‰")
    print("=" * 60)
    print()

    success = send_to_wechat_with_gist_link(
        account_name=summary_title,
        gist_url=gist_url,
        webhook_url=WEBHOOK_URL,
        articles=processed_articles
    )

    print()
    print("=" * 60)
    print("è¿è¡Œç»“æœ")
    print("=" * 60)
    print()

    if success:
        print("âœ… ä¼ä¸šå¾®ä¿¡æ¨é€æˆåŠŸï¼")

        # 8. ä¿å­˜å·²æ¨é€æ–‡ç« è®°å¿†
        print("\n[Step 8] ä¿å­˜å·²æ¨é€æ–‡ç« è®°å¿†...")
        for article in processed_articles:
            seen_links.add(article['link'])
        save_seen_articles(seen_links)
        print(f"   âœ… å·²ä¿å­˜ {len(processed_articles)} ç¯‡æ–‡ç« åˆ°è®°å¿†åº“")
        print(f"   ğŸ“ è®°å¿†æ–‡ä»¶: {SEEN_ARTICLES_FILE}")
        print(f"   ğŸ“Š æ€»è®°å¿†æ•°: {len(seen_links)} ç¯‡")
        print()

        print("ğŸ“Š å¤„ç†ç»Ÿè®¡ï¼š")
        print(f"   - è®¢é˜…å…¬ä¼—å·ï¼š{len(account_names)} ä¸ª")
        print(f"   - æœ¬æ¬¡æ–‡ç« ï¼š{len(processed_articles)} ç¯‡")
        print(f"   - å†å²è®°å¿†ï¼š{len(seen_links)} ç¯‡")
        print(f"   - Gist é“¾æ¥ï¼š{gist_url}")
        print(f"   - ä¼ä¸šå¾®ä¿¡ï¼šå¡ç‰‡æ¶ˆæ¯å·²å‘é€")
        print()
        print("ğŸ’¡ ç”¨æˆ·æ“ä½œï¼š")
        print("   1. æ‰“å¼€ä¼ä¸šå¾®ä¿¡")
        print("   2. ç‚¹å‡»å¡ç‰‡ä¸­çš„é“¾æ¥")
        print("   3. æŸ¥çœ‹å®Œæ•´çš„æ–‡ç« æ‘˜è¦ï¼ˆMarkdown æ ¼å¼ï¼‰")
        print()
        print("â° é€‚åˆ GitHub Actions å®šæ—¶ä»»åŠ¡ï¼š")
        print("   - å»ºè®®é¢‘ç‡ï¼šæ¯å°æ—¶è¿è¡Œä¸€æ¬¡")
        print("   - è®°å¿†æœºåˆ¶ï¼šè‡ªåŠ¨é¿å…é‡å¤æ¨é€")
        print("   - æ—¶é—´è¿‡æ»¤ï¼šåªå¤„ç†æœ€æ–°24å°æ—¶æ–‡ç« ")
    else:
        print("âŒ æ¨é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        print("âš ï¸ ç”±äºæ¨é€å¤±è´¥ï¼Œæœ¬æ¬¡æ–‡ç« æœªä¿å­˜åˆ°è®°å¿†åº“")
        print("   ä¸‹æ¬¡è¿è¡Œæ—¶ä¼šé‡æ–°å¤„ç†è¿™äº›æ–‡ç« ")

    print()


if __name__ == "__main__":
    main()
